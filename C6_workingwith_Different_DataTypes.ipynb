{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C6 : 6. Working with Different Types of Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp6       0      0 :::4040                 :::*                    LISTEN      21257/java          \r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('c6').getOrCreate()\n",
    "!netstat -anp |grep 4040 |grep LISTEN # check if the session is created and login to Spark console in localhost:4040 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV \n",
    "df = spark.read.load(path = \"/root/golive/Spark-The-Definitive-Guide/data/retail-data/by-day/2010-12-01.csv\"\\\n",
    "                     ,format = \"csv\"\\\n",
    "                     ,header = \"True\"\\\n",
    "                     ,inferschema = \"True\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('T1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \" select * from T1 Limit 2\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|new_col|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|      5|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|      5|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Literals lit function \n",
    "from pyspark.sql.functions import lit , expr \n",
    "df.withColumn(\"new_col\",lit(5)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|           new_col|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|some random String|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|some random String|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_col\",lit(\"some random String\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|new_col|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|    100|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|      0|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|    100|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|      0|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|      0|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://intellipaat.com/community/12495/pyspark-withcolumn-with-two-conditions-and-three-outcomes\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "newcolumn1 = expr(\"\"\"IF (Quantity > 2 and UnitPrice > 3 , 0 ,100)\"\"\")\n",
    "df.withColumn(\"new_col\",newcolumn1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|new_col|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|      0|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|      0|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newcolumn2 = when(col(\"Quantity\") > 2 ,0 ).when(col(\"UnitPrice\") >3 ,0).otherwise(100)\n",
    "#newcolumn3 = when(col(\"Quantity\") > 2  &  col(\"UnitPrice\") > 3 ,0).otherwise(100)\n",
    "df.withColumn(\"new_col\",newcolumn2 ).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOLEAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"InvoiceNo\"), col(\"Description\"))\\\n",
    ".where(col(\"InvoiceNo\") == 536365)\\\n",
    ".show(5,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SELECT * FROM dfTable WHERE StockCode in (\" DOT\") AND( UnitPrice > 600 OR instr( Description, \"POSTAGE\") > = 1)\n",
    "\n",
    "sql = 'select * from t1 where StockCode in (\"DOT\") and (UnitPrice > 600 or Description like \"%POSTAGE%\")'\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr \n",
    "\n",
    "filter1= col(\"UnitPrice\") > 600 \n",
    "filter2= instr(col(\"Description\"),\"POSTAGE\") >= 1 \n",
    "\n",
    "df.where(col(\"StockCode\") == \"DOT\")\\\n",
    ".where (filter1 | filter2).show()\n",
    "#.where (col(\"UnitPrice\") > 600 |  instr(col(\"Description\") ,\"POSTAGE\" )  >= 1 )\n",
    "\n",
    "        \n",
    "        \n",
    "#priceFilter = col(\"UnitPrice\") > 600 \n",
    "#descripFilter = instr( df.Description, \"POSTAGE\") >= 1 \n",
    "#df.where( df.StockCode.isin(\" DOT\")).where( priceFilter | descripFilter).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|         RealQuant|\n",
      "+----------+------------------+\n",
      "|   17850.0|234.08999999999997|\n",
      "|   17850.0|          413.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numerical calculation  \n",
    "\n",
    "sql = \"select  CustomerID, power(quantity* UnitPrice ,2) as RealQuant from t1 \"\n",
    "spark.sql(sql).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+\n",
      "|CustomerID|POWER((Quantity * UnitPrice), 2.0)|\n",
      "+----------+----------------------------------+\n",
      "|   17850.0|                234.08999999999997|\n",
      "|   17850.0|                          413.7156|\n",
      "+----------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pow\n",
    "\n",
    "RealQuant = pow(col(\"Quantity\") * col(\"UnitPrice\"),2)\n",
    "df.withColumn(\"RealQuant\",RealQuant).select(\"CustomerID\",RealQuant).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|UnitPrice|round(UnitPrice, 0)|\n",
      "+---------+-------------------+\n",
      "|     2.55|                3.0|\n",
      "|     3.39|                3.0|\n",
      "|     2.75|                3.0|\n",
      "|     3.39|                3.0|\n",
      "|     3.39|                3.0|\n",
      "|     7.65|                8.0|\n",
      "|     4.25|                4.0|\n",
      "|     1.85|                2.0|\n",
      "|     1.85|                2.0|\n",
      "|     1.69|                2.0|\n",
      "+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rounding Integer \n",
    "from pyspark.sql.functions import round, bround\n",
    "\n",
    "df.select(col(\"UnitPrice\"), round(col(\"UnitPrice\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|UnitPrice|bround(UnitPrice, 0)|\n",
      "+---------+--------------------+\n",
      "|     2.55|                 3.0|\n",
      "|     3.39|                 3.0|\n",
      "|     2.75|                 3.0|\n",
      "|     3.39|                 3.0|\n",
      "|     3.39|                 3.0|\n",
      "|     7.65|                 8.0|\n",
      "|     4.25|                 4.0|\n",
      "|     1.85|                 2.0|\n",
      "|     1.85|                 2.0|\n",
      "|     1.69|                 2.0|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"UnitPrice\"), bround(col(\"UnitPrice\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(UnitPrice)|\n",
      "+-----------------+\n",
      "|4.151946589446603|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id,max,mean,least,stddev,rank\n",
    "df.select(mean(col(\"UnitPrice\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings Manupulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|upper(Country)|\n",
      "+--------------+\n",
      "|UNITED KINGDOM|\n",
      "|UNITED KINGDOM|\n",
      "|UNITED KINGDOM|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1 : All srings upper case \n",
    "\n",
    "sql = \"select Country from t1\"\n",
    "sql = \"select upper(Country) from t1\"\n",
    "spark.sql(sql).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|upper(country)|\n",
      "+--------------+\n",
      "|UNITED KINGDOM|\n",
      "|UNITED KINGDOM|\n",
      "|UNITED KINGDOM|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you need upper - Look for it in funtions \n",
    "from pyspark.sql.functions import upper\n",
    "df.select(upper (col(\"country\"))).show(3)\n",
    "\n",
    "# NOTES \n",
    "# upper and initcap - same functionality \n",
    "# lower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+-----------------------+-----------------------+\n",
      "|ltrim( HELLLOOOO )|rtrim( HELLLOOOO )|trim( HELLLOOOO )|lpad( HELLOOOO , 20, *)|rpad( HELLOOOO , 20, *)|\n",
      "+------------------+------------------+-----------------+-----------------------+-----------------------+\n",
      "|        HELLLOOOO |         HELLLOOOO|        HELLLOOOO|   ********** HELLOOOO |    HELLOOOO **********|\n",
      "+------------------+------------------+-----------------+-----------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string functions \n",
    "\n",
    "sql = \"SELECT ltrim(' HELLLOOOO ')\\\n",
    ",rtrim(' HELLLOOOO ')\\\n",
    ",trim(' HELLLOOOO ')\\\n",
    ",lpad(' HELLOOOO ', 20, '*')\\\n",
    ",rpad(' HELLOOOO ', 20, '*')\\\n",
    "FROM t1\"\n",
    "\n",
    "spark.sql(sql).show(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|Description                        |new_desc                           |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |COLOR HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE METAL LANTERN                |COLOR METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |COLOR WOOLLY HOTTIE COLOR HEART.   |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |SET 7 BABUSHKA NESTING BOXES       |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |GLASS STAR FROSTED T-LIGHT HOLDER  |\n",
      "|HAND WARMER UNION JACK             |HAND WARMER UNION JACK             |\n",
      "|HAND WARMER RED POLKA DOT          |HAND WARMER COLOR POLKA DOT        |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |ASSORTED COLOUR BIRD ORNAMENT      |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String functions regex_replace regex_extract \n",
    "# Example regex_replace \n",
    "from pyspark.sql.functions import regexp_replace , regexp_extract\n",
    "\n",
    "df.withColumn(\"new_desc\" ,regexp_replace(col(\"Description\"),\"RED|BLACK|WHITE|GREEN|BLUE\",\"COLOR\"))\\\n",
    ".select(col(\"Description\"),col(\"new_desc\"))\\\n",
    ".show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |new_translate                     |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|WHITE H1NGING HE1RT T-LIGHT HOL4ER|\n",
      "|WHITE METAL LANTERN               |WHITE MET1L L1NTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function :translate \n",
    "# Literal Translation from one word to another . Good for data cleaning in case of special charecters \n",
    "# eg ABCD to 1234 (A-->1 , B--> 2 , C--> 3 and D-->4)\n",
    "from pyspark.sql.functions import translate \n",
    "\n",
    "sql = \" select Description , translate (Description ,'ABCD','1234') as new_translate from t1\"\n",
    "spark.sql(sql).show(2,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |new_translate                     |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|WHITE H1NGING HE1RT T-LIGHT HOL4ER|\n",
      "|WHITE METAL LANTERN               |WHITE MET1L L1NTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_translate\",translate (col(\"Description\"),'ABCD','1234'))\\\n",
    ".select(col(\"Description\"),col(\"new_translate\"))\\\n",
    ".show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Description: string (nullable = true)\n",
      " |-- new_extract: string (nullable = true)\n",
      "\n",
      "+--------------------+-----------+\n",
      "|         Description|new_extract|\n",
      "+--------------------+-----------+\n",
      "|WHITE HANGING HEA...|      WHITE|\n",
      "| WHITE METAL LANTERN|      WHITE|\n",
      "|CREAM CUPID HEART...|           |\n",
      "|KNITTED UNION FLA...|           |\n",
      "|RED WOOLLY HOTTIE...|        RED|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function : regexp_extract \n",
    "# E.g - Extract the first occurance of a string \n",
    "sql = \"select Description ,regexp_extract(Description,'(WHITE|BLACK|RED|GREEN)',1) as new_extract from t1 \"\n",
    "df_ext = spark.sql(sql)\n",
    "df_ext.printSchema()\n",
    "df_ext.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|         Description|new_extract|\n",
      "+--------------------+-----------+\n",
      "|WHITE HANGING HEA...|      WHITE|\n",
      "| WHITE METAL LANTERN|      WHITE|\n",
      "|CREAM CUPID HEART...|           |\n",
      "|KNITTED UNION FLA...|           |\n",
      "|RED WOOLLY HOTTIE...|        RED|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_string = \"(WHITE|BLACK|RED|GREEN)\"\n",
    "df.withColumn(\"new_extract\"\\\n",
    "              ,regexp_extract(col(\"Description\"),extract_string,1))\\\n",
    "              .select(col(\"Description\"),col(\"new_extract\")).show(5)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------+\n",
      "|Description                        |is_white|\n",
      "+-----------------------------------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |1       |\n",
      "|WHITE METAL LANTERN                |1       |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |0       |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|0       |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |19      |\n",
      "+-----------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------------------+--------+\n",
      "|Description                        |is_white|\n",
      "+-----------------------------------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |true    |\n",
      "|WHITE METAL LANTERN                |true    |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |false   |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|false   |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |true    |\n",
      "+-----------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function : instr \n",
    "# Requirement to check if string exists\n",
    "#instr returns the location of the word in the string (19 below represents the 19th word in the line)\n",
    "\n",
    "sql = \"select Description , instr(Description,'WHITE') as is_white from t1\"\n",
    "spark.sql(sql).show(5,False)\n",
    "\n",
    "sql = \"select Description , instr(Description,'WHITE') >=1  as is_white from t1\"\n",
    "spark.sql(sql).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|         Description|is_white|\n",
      "+--------------------+--------+\n",
      "|WHITE HANGING HEA...|    true|\n",
      "| WHITE METAL LANTERN|    true|\n",
      "|CREAM CUPID HEART...|   false|\n",
      "|KNITTED UNION FLA...|   false|\n",
      "|RED WOOLLY HOTTIE...|    true|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"Description\",\"(instr(Description,'WHITE') >=1) as is_white\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|is_ black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|    false|    true| false|   false|  false|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|    false|    true| false|   false|  false|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate \n",
    "simpleColors = [\" black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator( column, color_string): \n",
    "    return locate( color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
    "\n",
    "selectedColumns = [color_locator( df.Description, c) for c in simpleColors] \n",
    "selectedColumns.append( expr(\"*\")) # has to a be Column type \n",
    "\n",
    "df.select(*selectedColumns).show(2)\n",
    "df.select(*selectedColumns).where(expr(\"is_white or is_red\")).select(\"Description\").show(3,False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date and TIMESTAMPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "date_df=spark.range(10)\\\n",
    ".withColumn(\"date\" ,current_date())\\\n",
    ".withColumn(\"time\",current_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.createOrReplaceTempView(\"date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|future_date| past_date|\n",
      "+-----------+----------+\n",
      "| 2020-08-14|2020-08-04|\n",
      "| 2020-08-14|2020-08-04|\n",
      "| 2020-08-14|2020-08-04|\n",
      "| 2020-08-14|2020-08-04|\n",
      "| 2020-08-14|2020-08-04|\n",
      "+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADD and Subtract dates \n",
    "sql = \"select date_add(date ,5) as future_date , date_sub(date,5) as past_date from date_table\"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|future_date|past_date |\n",
      "+-----------+----------+\n",
      "|2020-08-14 |2020-08-04|\n",
      "|2020-08-14 |2020-08-04|\n",
      "+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub\n",
    "date_df.withColumn(\"future_date\",date_add(col(\"date\") ,5))\\\n",
    "       .withColumn(\"past_date\",date_sub(col(\"date\"),5))\\\n",
    "       .select(col(\"future_date\"),col(\"past_date\"))\\\n",
    "       .show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|today_Date|date_diff|months_bet|\n",
      "+----------+---------+----------+\n",
      "|2020-06-25|      366|      12.0|\n",
      "+----------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff ,months_between,to_date\n",
    "\n",
    "sql = \"select to_date('2020-06-25') as today_Date ,datediff('2020-06-25','2019-06-25') as date_diff  , months_between('2020-06-25','2019-06-25') as months_bet from date_table \"\n",
    "spark.sql(sql).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+----------+---------+----------+\n",
      "|id |date      |time                   |today_Date|date_diff|months_bet|\n",
      "+---+----------+-----------------------+----------+---------+----------+\n",
      "|0  |2020-08-09|2020-08-09 18:09:26.102|2020-06-25|366      |12.0      |\n",
      "+---+----------+-----------------------+----------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.withColumn(\"today_Date\",to_date(lit(\"2020-06-25\")))\\\n",
    "       .withColumn(\"date_diff\",datediff(to_date(lit(\"2020-06-25\")) , to_date(lit(\"2019-06-25\"))))\\\n",
    "       .withColumn(\"months_bet\", months_between(to_date(lit(\"2020-06-25\")) , to_date(lit(\"2019-06-25\"))))\\\n",
    "       .show(1,False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure Data format is correct define Dataformat \n",
    "DATEFORMAT = 'yyyy-dd-MM'   # only the month is in capitals - Spark gave an error when i typed YYYY insted of yyyy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+----------+---------+\n",
      "| id|      date|                time|today_Date|test_date|\n",
      "+---+----------+--------------------+----------+---------+\n",
      "|  0|2020-08-09|2020-08-09 18:09:...|2020-06-25|     null|\n",
      "|  1|2020-08-09|2020-08-09 18:09:...|2020-06-25|     null|\n",
      "+---+----------+--------------------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.withColumn(\"today_Date\", to_date(lit(\"2020-25-06\"),DATEFORMAT))\\\n",
    "       .withColumn(\"test_date\" , to_date(lit(\"2020-06-25\"),DATEFORMAT))\\\n",
    "       .show(2)\n",
    "       \n",
    "# TEST_DATE Column returns null - wrong format      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Handling NULL    \n",
    " # Spark dosen't enforce null constraint - Needs to be handled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+--------+--------+--------+----+----+------+------+\n",
      "|coalesce_1|coalesce_2|ifnull_1|ifnull_2|nullif_1|nullif_2|nvl1|nvl2|nvl2_1|nvl2_2|\n",
      "+----------+----------+--------+--------+--------+--------+----+----+------+------+\n",
      "|         1|         2|       1|       2|    null|       1|   1|   2|     1|     2|\n",
      "+----------+----------+--------+--------+--------+--------+----+----+------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #Functions :\n",
    "    # Coalesce - Returns first not null \n",
    "    # ifnull - Returns the second , if the first value is null  \n",
    "    # nullif - Returns NULL if two values are equal \n",
    "    # nvl - Returns second value if first is null \n",
    "    # nvl2 - Returns first not null --> second . First value == null , 3rd Value \n",
    "    \n",
    "sql = \"\"\" select \n",
    "coalesce( null , 1 ) coalesce_1,  coalesce( null , null, 2 ) coalesce_2 ,\n",
    "ifnull(null , 1) ifnull_1 ,  ifnull(2 , 3) ifnull_2,\n",
    "nullif(1,1) nullif_1 , nullif(1,2) nullif_2, \n",
    "nvl (null , 1) nvl1 , nvl(2,null ) nvl2,\n",
    " nvl2('x' ,1,2 ) nvl2_1, nvl2(null , 1,2) nvl2_2\n",
    "from t1 \"\"\"\n",
    "spark.sql(sql).show(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop() # drops the row if any of the column has null value  --> Defaults to df.na.drop('any')\n",
    "df.na.drop('all') # drops the row only if all values are null \n",
    "\n",
    "# sql \n",
    "sql = \"select * from t1 where description is not null \" # needs to handled at individual column in where clause \"\n",
    "list_col = ['StockCode','InvoiceNo']\n",
    "df.na.drop(\"all\",subset = list_col).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(5, subset = list_col) # for the subset columns all null values will be filled with value 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "|  C536548|    22244|3 HOOK HANGER MAG...|      -4|2010-12-01 14:33:00|     1.95|   12472.0|Germany|\n",
      "|  C536548|    20914|SET/5 RED RETROSP...|      -1|2010-12-01 14:33:00|     2.95|   12472.0|Germany|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536554|    84670|       null|      23|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   536414|    22139|       null|      56|2010-12-01 11:52:00|      0.0|      null|United Kingdom|\n",
      "|   536554|    84670|       null|      23|2010-12-01 14:35:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|    84832|ZINC WILLIE WINKI...|       3|2010-12-01 14:32:00|     1.66|      null|United Kingdom|\n",
      "|   536592|    84832|ZINC WILLIE WINKI...|       1|2010-12-01 17:06:00|     1.66|      null|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)\n",
    "df.orderBy(col(\"InvoiceNo\").desc()).show(2)\n",
    "df.orderBy(col(\"InvoiceNo\").asc(), col(\"Quantity\").desc()).show(2)\n",
    "df.orderBy(col(\"InvoiceNo\").asc(), col(\"Quantity\").desc()).show(2)\n",
    "df.orderBy(col(\"Description\").asc_nulls_first(), col(\"Quantity\").desc()).show(2)\n",
    "df.orderBy(col(\"Description\").desc_nulls_first(), col(\"Quantity\").desc()).show(2)\n",
    "df.orderBy(col(\"Description\").desc_nulls_last(), col(\"Quantity\").desc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPLEX DATA TYPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      struct_col|\n",
      "+----------------+\n",
      "|[536365, 85123A]|\n",
      "| [536365, 71053]|\n",
      "|[536365, 84406B]|\n",
      "+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STRUCT TYPE : Dataframe inside Dataframe \n",
    "from pyspark.sql.functions import struct \n",
    "struct_df = df.select(struct(col(\"InvoiceNo\"), col(\"StockCode\")).alias(\"struct_col\"))\n",
    "struct_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|InvoiceNo|\n",
      "+---------+\n",
      "|   536365|\n",
      "|   536365|\n",
      "+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "struct_df.select(col(\"struct_col.InvoiceNo\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function split \n",
    "\n",
    "sql = \"\"\"select split(Description,' ') from t1\"\"\"\n",
    "spark.sql(sql ).show(2)\n",
    "\n",
    "#split will breakdown into list of arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|split(Description,  , -1)[0]|\n",
      "+----------------------------+\n",
      "|                       WHITE|\n",
      "|                       WHITE|\n",
      "+----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"select split(Description,' ')[0] from t1\"\"\"\n",
    "spark.sql(sql ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|split_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "|       CREAM|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split \n",
    "df.select(split(col(\"Description\"), \" \")\\\n",
    ".alias(\"split_col\"))\\\n",
    ".selectExpr(\"split_col[0]\")\\\n",
    ".show(3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|size_split_arr|\n",
      "+--------------+\n",
      "|             5|\n",
      "|             3|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+\n",
      "|arr_contains|\n",
      "+------------+\n",
      "|        true|\n",
      "|        true|\n",
      "|       false|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Functions for arrays - \n",
    "# 1. Size \n",
    "# 2. Array_contains \n",
    "from pyspark.sql.functions import size , array_contains \n",
    "\n",
    "sql = \"\"\" \n",
    "select size(split(Description , \" \"))  as size_split_arr from t1 \n",
    "\"\"\"\n",
    "spark.sql(sql).show(2)\n",
    "\n",
    "sql = \"\"\"\n",
    "select array_contains(split(Description , \" \"), \"WHITE\") arr_contains from t1\n",
    "\"\"\"\n",
    "spark.sql(sql).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|arr_contains|\n",
      "+------------+\n",
      "|        true|\n",
      "|        true|\n",
      "|       false|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\")\\\n",
    ".alias(\"arr_contains\"))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    col|\n",
      "+-------+\n",
      "|  WHITE|\n",
      "|HANGING|\n",
      "|  HEART|\n",
      "|T-LIGHT|\n",
      "| HOLDER|\n",
      "|  WHITE|\n",
      "|  METAL|\n",
      "|LANTERN|\n",
      "|  CREAM|\n",
      "|  CUPID|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function : exploded ( splits coolum with array data into indiviual rows . Other columns gets duplicated )\n",
    "from pyspark.sql.functions import explode \n",
    "df.select(split(col(\"Description\"),\" \")\\\n",
    "          .alias(\"splitColumn\"))\\\n",
    "          .select(explode(col(\"splitColumn\")))\\\n",
    "          .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "| WHITE METAL LANTERN|   536365|   WHITE|\n",
      "| WHITE METAL LANTERN|   536365|   METAL|\n",
      "| WHITE METAL LANTERN|   536365| LANTERN|\n",
      "|CREAM CUPID HEART...|   536365|   CREAM|\n",
      "|CREAM CUPID HEART...|   536365|   CUPID|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html \n",
    "# for SQL syntax \n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT Description, InvoiceNo, exploded \n",
    "FROM (SELECT *, split( Description, \" \") as splitted \n",
    "      FROM t1) as v1 \n",
    "      LATERAL VIEW explode(v1.splitted) as exploded\n",
    "\"\"\"\n",
    "spark.sql(sql).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function MAPS \n",
    "# https://medium.com/@mrpowers/working-with-spark-arraytype-and-maptype-columns-4d85f3c8b2b3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|jsonString                                   |\n",
      "+---------------------------------------------+\n",
      "|{\" myJSONKey\" : {\" myJSONValue\" : [1, 2, 3]}}|\n",
      "+---------------------------------------------+\n",
      "\n",
      "+------+----+\n",
      "|column|  c0|\n",
      "+------+----+\n",
      "|  null|null|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working with JSON \n",
    "\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\" '{\" myJSONKey\" : {\" myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "jsonDF.show(1, False )\n",
    "\n",
    "from pyspark.sql.functions import get_json_object, json_tuple \n",
    "jsonDF.select( get_json_object( col(\"jsonString\"), \"$.myJSONKey.myJSONValue[0]\").alias(\"column\"), json_tuple( col(\"jsonString\"), \"myJSONKey\")).show( 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.function1(x)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def function1(x):\n",
    "    return x*x*x\n",
    "\n",
    "df1 =spark.range(1,100)\n",
    "df1.printSchema()\n",
    "\n",
    "spark.udf.register(\"reg_f\",function1,returnType=IntegerType())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"t2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+\n",
      "|reg_f(id)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        8|\n",
      "|       27|\n",
      "|       64|\n",
      "|      125|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"select id from t2\"\n",
    "spark.sql(sql).show(5)\n",
    "\n",
    "sql = \"select reg_f(id) from t2 \"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1|   2|\n",
      "|  3|   5|\n",
      "|  4|   4|\n",
      "|  3|   3|\n",
      "|  2|null|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2], [3,5],[4,4],[3,3],[2, None] ]\n",
    "columns = [\"a\" ,\"b\"]\n",
    "num_df = spark.createDataFrame(data,schema=columns)\n",
    "num_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.py_div(a, b)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "# python function \n",
    "def py_div(a,b):\n",
    "    if a and b:\n",
    "        return b/a \n",
    "    \n",
    "\n",
    "spark.udf.register(\"spark_div\",py_div , returnType=DoubleType())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+\n",
      "|  a|   b|   spark_div(a, b)|\n",
      "+---+----+------------------+\n",
      "|  1|   2|               2.0|\n",
      "|  3|   5|1.6666666666666667|\n",
      "|  4|   4|               1.0|\n",
      "|  3|   3|               1.0|\n",
      "|  2|null|              null|\n",
      "+---+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#num_df.withColumn(\"newdivvalue\",spark_div(col(\"a\"),col(\"b\")))\n",
    "#num_df.select(spark_div(col(\"a\"),col(\"b\")))\n",
    "\n",
    "num_df.createOrReplaceTempView(\"num_view\")\n",
    "sql = \"select a, b , spark_div(a,b) from num_view\"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+\n",
      "|  a|   b|       newdivvalue|\n",
      "+---+----+------------------+\n",
      "|  1|   2|               2.0|\n",
      "|  3|   5|1.6666666666666667|\n",
      "|  4|   4|               1.0|\n",
      "|  3|   3|               1.0|\n",
      "|  2|null|              null|\n",
      "+---+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_div = udf(py_div,DoubleType())\n",
    "num_df.withColumn(\"newdivvalue\",spark_div(col(\"a\"),col(\"b\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

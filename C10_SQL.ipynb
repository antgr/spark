{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C10 \n",
    "# Spark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1597293540960'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'c10'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'informix-test-01.phs.local'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '33922')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"c10\").getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Table \n",
    "# /root/golive/Spark-The-Definitive-Guide/data/flight-data/json\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF Exists flights\"\"\")\n",
    "\n",
    "create_json_table = \"\"\"\n",
    "CREATE TABLE flights ( DEST_COUNTRY_NAME STRING, \n",
    "                       ORIGIN_COUNTRY_NAME STRING, \n",
    "                       count LONG) \n",
    "USING JSON OPTIONS (path '/root/golive/Spark-The-Definitive-Guide/data/flight-data/json')\"\"\"\n",
    "\n",
    "spark.sql(create_json_table)\n",
    "\n",
    "spark.sql(\"select * from flights\").show(2)\n",
    "\n",
    "spark.sql(\"show tables\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "| default|    flights|      false|\n",
      "| default|flights_csv|      false|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column with Comments \n",
    "\n",
    "create_csv_table=\"\"\"CREATE TABLE flights_csv ( DEST_COUNTRY_NAME STRING, \n",
    "                          ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\", \n",
    "                          count LONG) \n",
    "USING csv OPTIONS (header true, path '/root/golive/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv')\"\"\"\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF Exists flights_csv\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(create_csv_table)\n",
    "\n",
    "spark.sql(\"select * from flights_csv\").show(2)\n",
    "\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----+-----------------+\n",
      "|      United States|   15|            Egypt|\n",
      "|            Romania|   15|    United States|\n",
      "+-------------------+-----+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| default|            flights|      false|\n",
      "| default|        flights_csv|      false|\n",
      "| default|partitioned_flights|      false|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_parquet = \"\"\"\n",
    "CREATE TABLE  IF NOT Exists partitioned_flights\n",
    "USING parquet \n",
    "PARTITIONED BY (DEST_COUNTRY_NAME) \n",
    "AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5\"\"\"\n",
    "\n",
    "spark.sql(create_parquet)\n",
    "\n",
    "spark.sql(\"select * from partitioned_flights\").show(2)\n",
    "\n",
    "spark.sql(\"show tables\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "| ORIGIN_COUNTRY_NAME|   string|   null|\n",
      "|               count|   bigint|   null|\n",
      "|   DEST_COUNTRY_NAME|   string|   null|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|   DEST_COUNTRY_NAME|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "+-------------------------------+\n",
      "|partition                      |\n",
      "+-------------------------------+\n",
      "|DEST_COUNTRY_NAME=Egypt        |\n",
      "|DEST_COUNTRY_NAME=United States|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe Table and show partition \n",
    "spark.sql(\"desc table partitioned_flights \").show()\n",
    "\n",
    "spark.sql(\"show partitions partitioned_flights \").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|partition                      |\n",
      "+-------------------------------+\n",
      "|DEST_COUNTRY_NAME=Egypt        |\n",
      "|DEST_COUNTRY_NAME=United States|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manage Partitions \n",
    "# REFRESH TABLE t1 & MSCK repair table t1 \n",
    "\n",
    "spark.sql(\"REFRESH table partitioned_flights \")\n",
    "\n",
    "spark.sql(\"MSCK repair table  partitioned_flights \")\n",
    "spark.sql(\"show partitions partitioned_flights \").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CACHE & UNCACHE \n",
    "\n",
    "spark.sql(\"CACHE table  partitioned_flights \")\n",
    "spark.sql(\"UNCACHE table  partitioned_flights \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIEWS \n",
    "# VIEWS _ Registerd in DB \n",
    "# TEMP VIEWS - Not registered \n",
    "# GLOBAL TEMP VIEWS - Not registerd . Available to spark sessions globally  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|      db1|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATBASES \n",
    "create_db = \"\"\"\n",
    "create database IF NOT Exists db1\n",
    "\"\"\"\n",
    "spark.sql (create_db)\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use db1 \")\n",
    "spark.sql(\"show tables \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SQL SYNTAX \n",
    "# https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPLEX DATA TYPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|country                 |count|\n",
      "+------------------------+-----+\n",
      "|[United States, Romania]|15   |\n",
      "|[United States, Croatia]|1    |\n",
      "|[United States, Ireland]|344  |\n",
      "|[Egypt, United States]  |15   |\n",
      "|[United States, India]  |62   |\n",
      "+------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# struct , MAP , Functions \n",
    "spark.sql(\"use default \")\n",
    "\n",
    "\n",
    "# STRUCT \n",
    "create_view = \"\"\"\n",
    "CREATE VIEW IF NOT EXISTS nested_data \n",
    "AS \n",
    "SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, \n",
    "count \n",
    "FROM flights\n",
    "\"\"\"\n",
    "spark.sql(create_view)\n",
    "sql = \"\"\"\n",
    "select * from nested_data\n",
    "\"\"\"\n",
    "spark.sql(sql).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|Romania            |15   |\n",
      "|Croatia            |1    |\n",
      "|Ireland            |344  |\n",
      "|United States      |15   |\n",
      "|India              |62   |\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using \".\" Operator#\n",
    "\n",
    "sql = \"\"\"\n",
    "select country.ORIGIN_COUNTRY_NAME , count  from nested_data\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   null|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   null|\n",
      "|              count|   bigint|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n",
      "+-----------------+--------------------+---------------+\n",
      "|DEST_COUNTRY_NAME|          list_count|    set_country|\n",
      "+-----------------+--------------------+---------------+\n",
      "|             Chad|                 [1]|[United States]|\n",
      "|         Anguilla|[21, 41, 22, 21, ...|[United States]|\n",
      "|         Paraguay|[90, 60, 75, 85, ...|[United States]|\n",
      "+-----------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LIST \n",
    "# COLLECT_LIST , COLLECT_SET , ARRAY \n",
    "\n",
    "sql = \"\"\"\n",
    "desc flights \n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "sql = \"\"\"\n",
    "select DEST_COUNTRY_NAME,\n",
    "COLLECT_LIST(count) as list_count,\n",
    "COLLECT_SET(ORIGIN_COUNTRY_NAME) as set_country\n",
    "from flights \n",
    "group by DEST_COUNTRY_NAME\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show(3)\n",
    "\n",
    "# Access first element in list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+---------------+\n",
      "|DEST_COUNTRY_NAME|list_count|    set_country|\n",
      "+-----------------+----------+---------------+\n",
      "|             Chad|         1|[United States]|\n",
      "|         Anguilla|        21|[United States]|\n",
      "|         Paraguay|        90|[United States]|\n",
      "+-----------------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First element in the list or set or Array \n",
    "sql = \"\"\"\n",
    "select DEST_COUNTRY_NAME,\n",
    "COLLECT_LIST(count)[0] as list_count,\n",
    "COLLECT_SET(ORIGIN_COUNTRY_NAME) as set_country\n",
    "from flights \n",
    "group by DEST_COUNTRY_NAME\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_view = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW flights_agg \n",
    "AS \n",
    "SELECT DEST_COUNTRY_NAME, \n",
    "collect_list( count) as collected_counts \n",
    "FROM flights \n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|DEST_COUNTRY_NAME|    collected_counts|\n",
      "+-----------------+--------------------+\n",
      "|             Chad|                 [1]|\n",
      "|         Anguilla|[21, 41, 22, 21, ...|\n",
      "|         Paraguay|[90, 60, 75, 85, ...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select * from flights_agg\n",
    "\"\"\"\n",
    "spark.sql(sql).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|col|DEST_COUNTRY_NAME|\n",
      "+---+-----------------+\n",
      "|  1|             Chad|\n",
      "| 21|         Anguilla|\n",
      "| 41|         Anguilla|\n",
      "| 22|         Anguilla|\n",
      "| 21|         Anguilla|\n",
      "| 34|         Anguilla|\n",
      "| 19|         Anguilla|\n",
      "| 90|         Paraguay|\n",
      "| 60|         Paraguay|\n",
      "| 75|         Paraguay|\n",
      "| 85|         Paraguay|\n",
      "| 90|         Paraguay|\n",
      "| 85|         Paraguay|\n",
      "|152|           Russia|\n",
      "|176|           Russia|\n",
      "|194|           Russia|\n",
      "|199|           Russia|\n",
      "|213|           Russia|\n",
      "|183|           Russia|\n",
      "|  1|            Yemen|\n",
      "+---+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode \n",
    "sql = \"\"\"\n",
    "select explode(collected_counts),\n",
    "DEST_COUNTRY_NAME\n",
    "From flights_agg\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|       !|\n",
      "|      !=|\n",
      "|       %|\n",
      "|       &|\n",
      "|       *|\n",
      "|       +|\n",
      "|       -|\n",
      "|       /|\n",
      "|       <|\n",
      "|      <=|\n",
      "|     <=>|\n",
      "|      <>|\n",
      "|       =|\n",
      "|      ==|\n",
      "|       >|\n",
      "|      >=|\n",
      "|       ^|\n",
      "|     abs|\n",
      "|    acos|\n",
      "|   acosh|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "show functions\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"c5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp6       0      0 :::4040                 :::*                    LISTEN      8998/java           \r\n"
     ]
    }
   ],
   "source": [
    "!netstat -anp |grep 4040 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.json('/root/golive/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.schema\n",
    "json_df.limit(4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in general for ETL , \n",
    "# its recommended to predefine the schema and not entirely rely on Spark to infer the schema on read.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymanualschema = StructType([ StructField(\" DEST_COUNTRY_NAME\", StringType(), True)\\\n",
    "                             , StructField(\" ORIGIN_COUNTRY_NAME\", StringType(), True)\\\n",
    "                             , StructField(\" count\", LongType(), False, metadata ={\" hello\":\" world\"})\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df1 = spark.read.json('/root/golive/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json',schema=mymanualschema ,multiLine=True)\n",
    "\n",
    "\n",
    "json_df2 = spark.read.format(\"json\")\\\n",
    ".schema(mymanualschema)\\\n",
    ".load('/root/golive/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField( DEST_COUNTRY_NAME,StringType,true),StructField( ORIGIN_COUNTRY_NAME,StringType,true),StructField( count,LongType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df1.schema\n",
    "json_df2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row( DEST_COUNTRY_NAME=None,  ORIGIN_COUNTRY_NAME=None,  count=None),\n",
       " Row( DEST_COUNTRY_NAME=None,  ORIGIN_COUNTRY_NAME=None,  count=None),\n",
       " Row( DEST_COUNTRY_NAME=None,  ORIGIN_COUNTRY_NAME=None,  count=None),\n",
       " Row( DEST_COUNTRY_NAME=None,  ORIGIN_COUNTRY_NAME=None,  count=None),\n",
       " Row( DEST_COUNTRY_NAME=None,  ORIGIN_COUNTRY_NAME=None,  count=None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df1.limit(5).collect()\n",
    "json_df2.limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions \n",
    "from pyspark.sql.functions import col \n",
    "x= col(\"mycolumnname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)  # A column outside DF ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   DEST_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|       United States|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|               Egypt|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|          Costa Rica|\n",
      "|             Senegal|\n",
      "|             Moldova|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|              Guyana|\n",
      "|               Malta|\n",
      "|            Anguilla|\n",
      "|             Bolivia|\n",
      "|       United States|\n",
      "|             Algeria|\n",
      "|Turks and Caicos ...|\n",
      "|       United States|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select('DEST_COUNTRY_NAME').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' DEST_COUNTRY_NAME', ' ORIGIN_COUNTRY_NAME', ' count']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newrow = Row('country1','India',33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.createOrReplaceTempView(\"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"select * from t1\"\n",
    "spark.sql(sql).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame  manually \n",
    "#steps:\n",
    "# 1. Create schema struture \n",
    "# 2. Create a row using Row\n",
    "\n",
    "#from pyspark.sql import Row\n",
    "#from pyspark.sql.types import StructType,StructField,StringType,LongType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([StructField(\"f1\", StringType(), True)\\\n",
    "                      ,StructField(\"f2\",StringType(), True)\\\n",
    "                      ,StructField(\"f3\",LongType(),True)\n",
    "                       ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrow = Row('Sajin','vk',99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([myrow],myschema)\n",
    "\n",
    "#myDf = spark.createDataFrame([ myrow], myschema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|   f1| f2| f3|\n",
      "+-----+---+---+\n",
      "|Sajin| vk| 99|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col ,expr ,column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|            Egypt|            Egypt|            Egypt|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(expr('DEST_COUNTRY_NAME'),\n",
    "              col ('DEST_COUNTRY_NAME'),\n",
    "              column('DEST_COUNTRY_NAME')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|           dd|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "|        Egypt|            Egypt|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(expr('DEST_COUNTRY_NAME as dd'),\n",
    "              'DEST_COUNTRY_NAME',\n",
    "              ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withincountry|\n",
      "+--------------------+-------------------+-----+-------------+\n",
      "|       United States|            Romania|   15|        false|\n",
      "|       United States|            Croatia|    1|        false|\n",
      "|       United States|            Ireland|  344|        false|\n",
      "|               Egypt|      United States|   15|        false|\n",
      "|       United States|              India|   62|        false|\n",
      "|       United States|          Singapore|    1|        false|\n",
      "|       United States|            Grenada|   62|        false|\n",
      "|          Costa Rica|      United States|  588|        false|\n",
      "|             Senegal|      United States|   40|        false|\n",
      "|             Moldova|      United States|    1|        false|\n",
      "|       United States|       Sint Maarten|  325|        false|\n",
      "|       United States|   Marshall Islands|   39|        false|\n",
      "|              Guyana|      United States|   64|        false|\n",
      "|               Malta|      United States|    1|        false|\n",
      "|            Anguilla|      United States|   41|        false|\n",
      "|             Bolivia|      United States|   30|        false|\n",
      "|       United States|           Paraguay|    6|        false|\n",
      "|             Algeria|      United States|    4|        false|\n",
      "|Turks and Caicos ...|      United States|  230|        false|\n",
      "|       United States|          Gibraltar|    1|        false|\n",
      "+--------------------+-------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withincountry\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withincountry|\n",
      "+--------------------+-------------------+-----+-------------+\n",
      "|       United States|            Romania|   15|        false|\n",
      "|       United States|            Croatia|    1|        false|\n",
      "|       United States|            Ireland|  344|        false|\n",
      "|               Egypt|      United States|   15|        false|\n",
      "|       United States|              India|   62|        false|\n",
      "|       United States|          Singapore|    1|        false|\n",
      "|       United States|            Grenada|   62|        false|\n",
      "|          Costa Rica|      United States|  588|        false|\n",
      "|             Senegal|      United States|   40|        false|\n",
      "|             Moldova|      United States|    1|        false|\n",
      "|       United States|       Sint Maarten|  325|        false|\n",
      "|       United States|   Marshall Islands|   39|        false|\n",
      "|              Guyana|      United States|   64|        false|\n",
      "|               Malta|      United States|    1|        false|\n",
      "|            Anguilla|      United States|   41|        false|\n",
      "|             Bolivia|      United States|   30|        false|\n",
      "|       United States|           Paraguay|    6|        false|\n",
      "|             Algeria|      United States|    4|        false|\n",
      "|Turks and Caicos ...|      United States|  230|        false|\n",
      "|       United States|          Gibraltar|    1|        false|\n",
      "+--------------------+-------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"select * , (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withincountry from t1  \"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import avg , count\n",
    "json_df.selectExpr(\"avg(count)\"\\\n",
    "                   ,\"count(distinct(DEST_COUNTRY_NAME))\"\\\n",
    "                   ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"select avg(count), count(distinct(DEST_COUNTRY_NAME)) from t1  \"\n",
    "#sql = \"select count from t1\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newcolumn|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#literals \n",
    "\n",
    "sql = \"select * , 1 as newcolumn from t1\"\n",
    "spark.sql(sql).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newcolumn|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.selectExpr(\"*\"\\\n",
    "                   , \"1 as newcolumn\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newcolumn|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(expr(\"*\")\\\n",
    "                   , lit(1).alias(\"newcolumn\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newcolumn|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        2|\n",
      "|    United States|            Croatia|    1|        2|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADDING Columns :\n",
    "# withColumn \n",
    "\n",
    "json_df.withColumn(\"newcolumn\",lit(\"2\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newcolumn|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|    false|\n",
      "|    United States|            Croatia|    1|    false|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.withColumn(\"newcolumn\",expr(\"DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|         DEST|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming columns \n",
    "json_df.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"DEST\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "+-----------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#removing COLUMS df.drop(\"col1\",\"col2\")\n",
    "json_df.drop(\"count\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast is used for type conversion \n",
    "\n",
    "#CAST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering ROWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|           Mexico|      United States|  7140|\n",
      "|    United States| Dominican Republic|  1420|\n",
      "|    United States|      United States|370002|\n",
      "|          Germany|      United States|  1468|\n",
      "|           Canada|      United States|  8399|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#where and #filter \n",
    "json_df.where(\"count > 1000\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|           Mexico|      United States|  7140|\n",
      "|    United States| Dominican Republic|  1420|\n",
      "|    United States|      United States|370002|\n",
      "|          Germany|      United States|  1468|\n",
      "|           Canada|      United States|  8399|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"select * from t1 where count > 1000\"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+------------------+-------------------+-----+\n",
      "|            Mexico|      United States| 7140|\n",
      "|           Germany|      United States| 1468|\n",
      "|            Canada|      United States| 8399|\n",
      "|Dominican Republic|      United States| 1353|\n",
      "|             Japan|      United States| 1548|\n",
      "+------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.where(\"count > 1000\").where(\"DEST_COUNTRY_NAME != 'United States'\").show(5)\n",
    "\n",
    "#same as  SQL AND \n",
    "# select * from t1 where count > 1000 and Dest_country_Name != 'United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count_distinct|\n",
      "+--------------+\n",
      "|           256|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct DEST_COUNTRY_NAME and ORIGIN_COUNTRY_NAME \n",
    "\n",
    "sql = \"select count(distinct(DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME)) as count_distinct from t1 \"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df\n",
    "seed = 5 \n",
    "withReplacement = False \n",
    "fraction = 0.5 \n",
    "json_df.sample( withReplacement, fraction, seed).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf=json_df.randomSplit([0.25 ,0.75],seed=None)\n",
    "newdf[0].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort data \n",
    "# Sort or Order BY \n",
    "from pyspark.sql.functions import asc, desc \n",
    "\n",
    "#Advanced Options \n",
    "#asc_nulls_first,\n",
    "#desc_nulls_first, \n",
    "#asc_nulls_last, \n",
    "#desc_nulls_last\n",
    "\n",
    "\n",
    "\n",
    "json_df.orderBy(expr(\"count desc\")).show(5)\n",
    "\n",
    "#SQL Equivalent \n",
    "# sql = \"select * from t1 order by count desc \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REpartition only when the number of partitions needs to be increased \n",
    "new_repartion_df = json_df.repartition(5)\n",
    "new_repartion_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.repartition(5,col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

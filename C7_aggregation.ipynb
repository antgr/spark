{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp6       0      0 :::4040                 :::*                    LISTEN      14404/java          \r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"C7\").getOrCreate()\n",
    "!netstat -anp |grep 4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/*.csv\")\\\n",
    ".coalesce(5)\n",
    "\n",
    "df.show(5)\n",
    "df.createOrReplaceTempView(\"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|countall|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|countall|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BASIC Aggregation Functions \n",
    "# Count - Aggregate Functions \n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "sql = \"\"\"\n",
    "select count(*) as countall from t1 \n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "df.selectExpr(\"count(StockCode) as countall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(StockCode#17)])\n",
      "+- Exchange SinglePartition, true, [id=#115]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(StockCode#17)])\n",
      "      +- Coalesce 5\n",
      "         +- FileScan csv [StockCode#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/online-retail..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<StockCode:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"count(StockCode) as countall\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count_distinct|\n",
      "+--------------+\n",
      "|          4070|\n",
      "+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(distinct StockCode#17)])\n",
      "+- Exchange SinglePartition, true, [id=#196]\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct StockCode#17)])\n",
      "      +- *(2) HashAggregate(keys=[StockCode#17], functions=[])\n",
      "         +- Exchange hashpartitioning(StockCode#17, 200), true, [id=#191]\n",
      "            +- *(1) HashAggregate(keys=[StockCode#17], functions=[])\n",
      "               +- Coalesce 5\n",
      "                  +- FileScan csv [StockCode#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/online-retail..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<StockCode:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountDistinct Aggregate Functions \n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\").alias(\"count_distinct\")).show()\n",
    "df.select(countDistinct(\"StockCode\").alias(\"count_distinct\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(distinct StockCode#17)])\n",
      "+- Exchange SinglePartition, true, [id=#277]\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct StockCode#17)])\n",
      "      +- *(2) HashAggregate(keys=[StockCode#17], functions=[])\n",
      "         +- Exchange hashpartitioning(StockCode#17, 200), true, [id=#272]\n",
      "            +- *(1) HashAggregate(keys=[StockCode#17], functions=[])\n",
      "               +- Coalesce 5\n",
      "                  +- FileScan csv [StockCode#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/online-retail..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<StockCode:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "select count(distinct(StockCode)) from t1\"\"\"\n",
    "spark.sql(sql).show()\n",
    "spark.sql(sql).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "HashAggregate(keys=[], functions=[approx_count_distinct(StockCode#17, 0.1, 0, 0)])\n",
      "+- Exchange SinglePartition, true, [id=#315]\n",
      "   +- HashAggregate(keys=[], functions=[partial_approx_count_distinct(StockCode#17, 0.1, 0, 0)])\n",
      "      +- Coalesce 5\n",
      "         +- FileScan csv [StockCode#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/online-retail..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<StockCode:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approxCountDistinct , approx_count_distinct\n",
    "sql = \"\"\"\n",
    "select approx_count_distinct(StockCode,0.1) from t1\"\"\"\n",
    "spark.sql(sql).show()\n",
    "spark.sql(sql).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(col(\"StockCode\"),0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n",
      "+-----------------------+----------------------+\n",
      "|first(stockcode, false)|last(stockcode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first , last\n",
    "from pyspark.sql.functions import first , last \n",
    "df.select(first(col(\"StockCode\")), last(col(\"StockCode\"))).show()\n",
    "\n",
    "sql = \"\"\" select first(stockcode) , last(stockcode) from t1\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(StockCode)|max(StockCode)|\n",
      "+--------------+--------------+\n",
      "|         10002|             m|\n",
      "+--------------+--------------+\n",
      "\n",
      "+--------------+--------------+\n",
      "|min(stockcode)|max(stockcode)|\n",
      "+--------------+--------------+\n",
      "|         10002|             m|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max \n",
    "df.select(min(col(\"StockCode\")), max(col(\"StockCode\"))).show()\n",
    "\n",
    "sql = \"\"\" select min(stockcode) , max(stockcode) from t1\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions like sum , average , mean works in similar fashion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------------------+------------------+\n",
      "|          var_pop|         var_samp|           std_pop|          std_samp|\n",
      "+-----------------+-----------------+------------------+------------------+\n",
      "|47559.30364660923|47559.39140929892|218.08095663447835|218.08115785023455|\n",
      "+-----------------+-----------------+------------------+------------------+\n",
      "\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"SELECT var_pop( Quantity) var_pop ,var_samp( Quantity) var_samp , stddev_pop( Quantity) std_pop,\n",
    "stddev_samp( Quantity)  std_samp\n",
    "FROM t1\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "from pyspark.sql.functions import var_samp, var_pop , stddev_samp, stddev_pop\n",
    "df.select(var_pop(col(\"Quantity\"))\\\n",
    "          ,var_samp(col(\"Quantity\"))\\\n",
    "          ,stddev_pop(col(\"Quantity\"))\\\n",
    "          ,stddev_samp(col(\"Quantity\"))\\\n",
    "         ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|skewness(CAST(Quantity AS DOUBLE))|kurtosis(CAST(Quantity AS DOUBLE))|\n",
      "+----------------------------------+----------------------------------+\n",
      "|              -0.26407557610528376|                119768.05495530753|\n",
      "+----------------------------------+----------------------------------+\n",
      "\n",
      "+--------------------+------------------+------------------+\n",
      "|         correlation|           cv_samp|            cv_pop|\n",
      "+--------------------+------------------+------------------+\n",
      "|4.912186085640497E-4|1052.7280543915997|1052.7260778754955|\n",
      "+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SKEWNESS , kurtosis\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT skewness( Quantity), kurtosis( Quantity) FROM t1\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n",
    "\n",
    "# COVARIANCE , CORRELATION \n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT corr( InvoiceNo, Quantity) as correlation , covar_samp( InvoiceNo, Quantity) as cv_samp, \n",
    "covar_pop( InvoiceNo, Quantity) as cv_pop FROM t1\n",
    "\"\"\"\n",
    "spark.sql(sql).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list,collect_set\n",
    "df.printSchema()\n",
    "df.select(collect_list(col(\"Country\"))\\\n",
    "          ,collect_set(col(\"Country\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|InvoiceNo|CustomerID|count(1)|\n",
      "+---------+----------+--------+\n",
      "|   536846|     14573|      76|\n",
      "|   537026|     12395|      12|\n",
      "|   537883|     14437|       5|\n",
      "+---------+----------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+----------+-------------+\n",
      "|InvoiceNo|CustomerID|count_Invoice|\n",
      "+---------+----------+-------------+\n",
      "|   536846|     14573|           76|\n",
      "|   537026|     12395|           12|\n",
      "|   537883|     14437|            5|\n",
      "+---------+----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by InvoiceNo & CustomerID - Aggregate Function : Count \n",
    "from pyspark.sql.functions import count\n",
    "sql = \"\"\"\n",
    "select InvoiceNo, CustomerID, Count(*) from t1 group by InvoiceNo, CustomerID\n",
    "\"\"\"\n",
    "spark.sql(sql).show(3)\n",
    "\n",
    "df.groupBy(col(\"InvoiceNo\"),col(\"CustomerID\"))\\\n",
    ".agg(count(\"InvoiceNo\").alias(\"count_Invoice\"))\\\n",
    ".show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+--------------+\n",
      "|InvoiceNo|CustomerID|count_Invoice|expr_count_inv|\n",
      "+---------+----------+-------------+--------------+\n",
      "|   536846|     14573|           76|            76|\n",
      "|   537026|     12395|           12|            12|\n",
      "|   537883|     14437|            5|             5|\n",
      "+---------+----------+-------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['InvoiceNo, 'CustomerID], [unresolvedalias('InvoiceNo, None), unresolvedalias('CustomerID, None), count('InvoiceNo) AS count_Invoice#1815, 'count('InvoiceNo) AS expr_count_inv#1816]\n",
      "+- Repartition 5, false\n",
      "   +- Relation[InvoiceNo#16,StockCode#17,Description#18,Quantity#19,InvoiceDate#20,UnitPrice#21,CustomerID#22,Country#23] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InvoiceNo: string, CustomerID: int, count_Invoice: bigint, expr_count_inv: bigint\n",
      "Aggregate [InvoiceNo#16, CustomerID#22], [InvoiceNo#16, CustomerID#22, count(InvoiceNo#16) AS count_Invoice#1815L, count(InvoiceNo#16) AS expr_count_inv#1816L]\n",
      "+- Repartition 5, false\n",
      "   +- Relation[InvoiceNo#16,StockCode#17,Description#18,Quantity#19,InvoiceDate#20,UnitPrice#21,CustomerID#22,Country#23] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [InvoiceNo#16, CustomerID#22], [InvoiceNo#16, CustomerID#22, count(InvoiceNo#16) AS count_Invoice#1815L, count(InvoiceNo#16) AS expr_count_inv#1816L]\n",
      "+- Repartition 5, false\n",
      "   +- Project [InvoiceNo#16, CustomerID#22]\n",
      "      +- Relation[InvoiceNo#16,StockCode#17,Description#18,Quantity#19,InvoiceDate#20,UnitPrice#21,CustomerID#22,Country#23] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[InvoiceNo#16, CustomerID#22], functions=[count(InvoiceNo#16)], output=[InvoiceNo#16, CustomerID#22, count_Invoice#1815L, expr_count_inv#1816L])\n",
      "+- Exchange hashpartitioning(InvoiceNo#16, CustomerID#22, 200), true, [id=#661]\n",
      "   +- *(1) HashAggregate(keys=[InvoiceNo#16, CustomerID#22], functions=[partial_count(InvoiceNo#16)], output=[InvoiceNo#16, CustomerID#22, count#1823L])\n",
      "      +- Coalesce 5\n",
      "         +- FileScan csv [InvoiceNo#16,CustomerID#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/root/golive/Spark-The-Definitive-Guide/data/retail-data/all/online-retail..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,CustomerID:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by expression \n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.groupBy(col(\"InvoiceNo\"),col(\"CustomerID\"))\\\n",
    ".agg(count(\"InvoiceNo\").alias(\"count_Invoice\"), expr(\"count(InvoiceNo)\").alias(\"expr_count_inv\"))\\\n",
    ".show(3)\n",
    "\n",
    "\n",
    "df.groupBy(col(\"InvoiceNo\"),col(\"CustomerID\"))\\\n",
    ".agg(count(\"InvoiceNo\").alias(\"count_Invoice\"), expr(\"count(InvoiceNo)\").alias(\"expr_count_inv\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------------------------+\n",
      "|InvoiceNO|     avg(Quantity)|stddev_pop(CAST(Quantity AS DOUBLE))|\n",
      "+---------+------------------+------------------------------------+\n",
      "|   536596|               1.5|                  1.1180339887498947|\n",
      "|   536938|33.142857142857146|                  20.698023172885524|\n",
      "|   537252|              31.0|                                 0.0|\n",
      "+---------+------------------+------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by using MAP ( Not sure what is MAP here though)\n",
    "from pyspark.sql.functions import stddev_pop\n",
    "sql = \"\"\"\n",
    "select InvoiceNO, avg(Quantity), stddev_pop(Quantity)\n",
    "from t1\n",
    "Group by InvoiceNO\"\"\"\n",
    "\n",
    "spark.sql(sql).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "|InvoiceNO|      avg_quantity|     std_dev_quant|\n",
      "+---------+------------------+------------------+\n",
      "|   536596|               1.5|1.1180339887498947|\n",
      "|   536938|33.142857142857146|20.698023172885524|\n",
      "|   537252|              31.0|               0.0|\n",
      "+---------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg , stddev_pop\n",
    "df.groupBy(col(\"InvoiceNO\"))\\\n",
    ".agg(avg(\"Quantity\").alias(\"avg_quantity\"), stddev_pop(\"Quantity\").alias(\"std_dev_quant\"))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |date      |\n",
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|2010-12-01|\n",
      "|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01|\n",
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+----+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|date|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+----+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark supports 3 kinds of window functions :\n",
    "# 1. Aggregate function \n",
    "# 2. Ranking  function\n",
    "# 3. Analytical function \n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "from pyspark.sql.functions import to_date\n",
    "df.show(2)\n",
    "dfwithdate =df.withColumn(\"date\",to_date(\"InvoiceDate\",\"MM/d/yyyy H:mm\"))\n",
    "dfwithdate.select(\"*\").show(2,False)\n",
    "\n",
    "dfwithdate.where(\"date is null\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 : create window specification \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowspec = Window\\\n",
    ".partitionBy(col(\"CustomerID\"), col(\"date\"))\\\n",
    ".orderBy(col(\"Quantity\").desc())\\\n",
    ".rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 - Use one of Aggregate , Ranking or Analytical function on the window specification \n",
    "#Keyword - over \n",
    "from pyspark.sql.functions import max , min\n",
    "max_quant_window = max(col(\"Quantity\")).over(windowspec)\n",
    "\n",
    "# Aggregate Function OVER \n",
    "\n",
    "#dfwithdate.withColumn(\"max_quant_window\",max(col(\"Quantity\")).over(windowspec))\\\n",
    "#.show()\n",
    "\n",
    "#df_new.show(2)\n",
    "#maxPurchaseQuantity = max(col(\"Quantity\")).over( windowSpec)\n",
    "#df_new.orderBy(col(\"max_quant_window\").desc()).show(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------------------+-----------------+-----------+\n",
      "|CustomerID|      date|quantity|max_quantity_window|Dense_Rank_window|Rank_window|\n",
      "+----------+----------+--------+-------------------+-----------------+-----------+\n",
      "|     18245|2010-12-19|      24|                 24|                1|          1|\n",
      "|     18245|2010-12-19|      24|                 24|                1|          1|\n",
      "|     18245|2010-12-19|      24|                 24|                1|          1|\n",
      "|     18245|2010-12-19|      12|                 24|                2|          4|\n",
      "|     18245|2010-12-19|      12|                 24|                2|          4|\n",
      "+----------+----------+--------+-------------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ranking function Over WindowSpecification \n",
    "\n",
    "from pyspark.sql.functions import rank,dense_rank\n",
    "purchaseDenseRank = dense_rank().over(windowspec)\n",
    "type(purchaseDenseRank)\n",
    "purchaseRank = rank().over(windowspec)\n",
    "\n",
    "#dfwithdate.printSchema()\n",
    "dfwithdate.where(\"CustomerID is not null \")\\\n",
    ".orderBy(col(\"CustomerID\").desc())\\\n",
    ".select(col(\"CustomerID\"), col(\"date\"), col(\"quantity\")\\\n",
    "        ,max_quant_window.alias(\"max_quantity_window\")\\\n",
    "        ,purchaseDenseRank.alias(\"Dense_Rank_window\")\\\n",
    "        ,purchaseRank.alias(\"Rank_window\"))\\\n",
    ".show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----------+-----------+-----------------+\n",
      "|customerID|      date|quantity|maxPurchase|rank_window|Dense_rank_window|\n",
      "+----------+----------+--------+-----------+-----------+-----------------+\n",
      "|     18287|2011-05-22|      30|         60|          4|                4|\n",
      "|     18287|2011-05-22|      48|         60|          2|                2|\n",
      "|     18287|2011-05-22|      36|         60|          3|                3|\n",
      "|     18287|2011-05-22|      60|         60|          1|                1|\n",
      "|     18287|2011-05-22|      24|         60|          6|                5|\n",
      "+----------+----------+--------+-----------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfwithdate.createOrReplaceTempView(\"t_dfwithdate\")\n",
    "sql = \"\"\"\n",
    "select  customerID , \n",
    "date ,\n",
    "quantity ,\n",
    "max( Quantity) OVER (PARTITION BY CustomerId, date \n",
    "                    ORDER BY Quantity DESC NULLS LAST \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as maxPurchase,\n",
    "rank(Quantity) OVER (PARTITION BY CustomerId, date \n",
    "                    ORDER BY Quantity DESC NULLS LAST \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rank_window ,\n",
    "                    \n",
    "dense_rank(Quantity) OVER (PARTITION BY CustomerId, date \n",
    "                    ORDER BY Quantity DESC NULLS LAST \n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as Dense_rank_window\n",
    "\n",
    "from t_dfwithdate\n",
    "where CustomerID is not null \n",
    "order by CustomerID desc \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|     null|         1586|\n",
      "|     18283|     null|         1397|\n",
      "|     18282|     null|           98|\n",
      "|     18281|     null|           54|\n",
      "|     18280|     null|           45|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping Sets \n",
    "\n",
    "dfNoNull = dfwithdate.drop() \n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT CustomerId, stockCode, sum( Quantity) \n",
    "FROM dfNoNull \n",
    "GROUP BY customerId, stockCode\n",
    "order by customerid desc , stockcode desc \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show(5)\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT CustomerId, stockCode, sum( Quantity) \n",
    "FROM dfNoNull \n",
    "GROUP BY customerId, stockCode grouping sets(customerId, stockCode , ())\n",
    "order by customerid desc , stockcode desc \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://oracle-base.com/articles/misc/rollup-cube-grouping-functions-and-grouping-sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
